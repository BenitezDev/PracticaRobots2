{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robots Autónomos. Mapas Topológicos visuales\n",
    "\n",
    "Autores:\n",
    "    Alejandro Benítez López, Elena Benito Frey, Mario González Carbayo, Isidro López Dominguez, Blanca Martínez Donoso y Ángel Pavón Pérez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las capas que tendrá el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadModel():\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Conv2D(96, (11,11),activation='relu', input_shape=(128,128,3)),\n",
    "            keras.layers.MaxPooling2D(2,2),\n",
    "            keras.layers.Conv2D(256, (7,7),activation='relu'),\n",
    "            keras.layers.MaxPooling2D(2,2),\n",
    "            keras.layers.Conv2D(384, (5,5),activation='relu'),\n",
    "            keras.layers.MaxPooling2D(2,2),\n",
    "            keras.layers.Conv2D(384, (5,5),activation='relu'),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1024),\n",
    "            keras.layers.Dense(1024),\n",
    "            keras.layers.Dense(9, activation=\"softmax\") \n",
    "        ])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(image):\n",
    "    stdR = np.std(image[:,:,0]) #Calculamos la varianza para cada espacio de color\n",
    "    stdG = np.std(image[:,:,1])\n",
    "    stdB = np.std(image[:,:,2])\n",
    "    \n",
    "    R = np.mean(image[:,:,0]) #La mediana de cada espacio de color\n",
    "    G = np.mean(image[:,:,1])\n",
    "    B = np.mean(image[:,:,2])\n",
    "    \n",
    "    image[:,:,0] = image[:,:,0] - R\n",
    "    image[:,:,1] = image[:,:,1] - G\n",
    "    image[:,:,2] = image[:,:,2] - B\n",
    "    \n",
    "    image[:,:,0] = image[:,:,0]/stdR\n",
    "    image[:,:,1] = image[:,:,1]/stdG\n",
    "    image[:,:,2] = image[:,:,2]/stdB\n",
    "    \n",
    "    minimoR = np.min(image[:,:,0])\n",
    "    minimoG = np.min(image[:,:,1])\n",
    "    minimoB = np.min(image[:,:,2])\n",
    "    \n",
    "    image[:,:,0] = image[:,:,0]-minimoR # Restamos el valor minimo de cada espacio para que el valor mas bajo sea 0\n",
    "    image[:,:,1] = image[:,:,1]-minimoG\n",
    "    image[:,:,2] = image[:,:,2]-minimoB\n",
    "    \n",
    "    maximoR = np.max(image[:,:,0]) \n",
    "    maximoG = np.max(image[:,:,1])\n",
    "    maximoB = np.max(image[:,:,2])\n",
    "    \n",
    "    factorR = 1/maximoR #Calculamos el factor escala por el que hay que multiplicar cada espacio para que su valor maximo sea 1\n",
    "    factorG = 1/maximoG\n",
    "    factorB = 1/maximoB\n",
    "    \n",
    "    image[:,:,0] = image[:,:,0] * factorR #Operamos de modo que cada color tiene sus valores comprendidos entre [0,1]\n",
    "    image[:,:,1] = image[:,:,1] * factorG\n",
    "    image[:,:,2] = image[:,:,2] * factorB\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos las rutas y constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimension = (128,128)\n",
    "\n",
    "video_route = \"./landmarks_videos/landmarks_{:}.avi\" #Esto se usaba para la carga de imagenes de training, pero funcionaba mal\n",
    "video = cv2.VideoCapture(video_route)                #y, como se comenta en la memoria, usamos el video de test para entrenar\n",
    "\n",
    "video_route_test = \"./landmarks_videos/final_test_video.mp4\"\n",
    "video_test = cv2.VideoCapture(video_route_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones de carga de frames de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrameNumber(): #Devueve el numero de frames\n",
    "    return int(video_test.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "def getFrames(number): #Cargamos number imagenes consecutivas sobre el video_test\n",
    "    frames=[]\n",
    "    for i in range(number):\n",
    "        ret, frame = video_test.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame = frame.resize(output_dimension, Image.ANTIALIAS)\n",
    "        frame = np.asarray( frame, dtype=\"float32\" )\n",
    "        frame = Normalize(frame)\n",
    "        frames.append(frame)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones de carga de frames para entrenamiento y development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomNumbers(end, number):\n",
    "    res = []\n",
    "    for i in range(number):\n",
    "        num = random.randint(0, end-1)\n",
    "        while num in res:\n",
    "            num = random.randint(0, end-1)\n",
    "        res.append(num)\n",
    "    res.sort()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def getImagesFromTest(rands): \n",
    "    frames =[]\n",
    "    video_route = \"./landmarks_videos/final_test_video.mp4\"\n",
    "    video = cv2.VideoCapture(video_route)    \n",
    "    for i in range(int(video.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        ret, frame = video.read()\n",
    "        if i in rands:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            frame = frame.resize(output_dimension, Image.ANTIALIAS)\n",
    "            frame = np.asarray( frame, dtype=\"float32\" )\n",
    "            frame = Normalize(frame)            \n",
    "            frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def developmentTest(number): #Devuelve number frames aleatorios del video de test\n",
    "    gt = pickle.load(open(\"clasificacion_frames.sav\", 'rb'))\n",
    "    labels = []\n",
    "    rands = generateRandomNumbers(len(gt), number)\n",
    "    dev = getImagesFromTest(rands)\n",
    "    for i in rands:\n",
    "        labels.append(int(gt[i]))\n",
    "    return dev, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train y test de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = developmentTest(100) #We get the training set from the test video\n",
    "\n",
    "x = np.asarray(x)#Change them from list to np array\n",
    "y = np.asarray(y) \n",
    "\n",
    "x_dev, y_dev = developmentTest(100) #We get the development set from the test video\n",
    "\n",
    "x_dev = np.asarray(x_dev)\n",
    "y_dev = np.asarray(y_dev) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos y compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow15\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LoadModel() #We get the model\n",
    "\n",
    "model.compile(optimizer=\"Adam\",  #Compile it\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 45.3849 - sparse_categorical_accuracy: 0.1000\n",
      "0.1\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.3242 - sparse_categorical_accuracy: 0.1800\n",
      "0.1\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.3091 - sparse_categorical_accuracy: 0.2000\n",
      "0.28\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.3193 - sparse_categorical_accuracy: 0.1900\n",
      "0.28\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 9.7030 - sparse_categorical_accuracy: 0.1700\n",
      "0.1\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.4396 - sparse_categorical_accuracy: 0.1300\n",
      "0.16\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.1613 - sparse_categorical_accuracy: 0.1300\n",
      "0.3\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.1753 - sparse_categorical_accuracy: 0.0800\n",
      "0.28\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.1571 - sparse_categorical_accuracy: 0.1600\n",
      "0.12\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.1013 - sparse_categorical_accuracy: 0.2500\n",
      "0.3\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.0534 - sparse_categorical_accuracy: 0.2300\n",
      "0.24\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.8886 - sparse_categorical_accuracy: 0.3200\n",
      "0.12\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.8944 - sparse_categorical_accuracy: 0.3400\n",
      "0.38\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.3707 - sparse_categorical_accuracy: 0.2800\n",
      "0.12\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.1398 - sparse_categorical_accuracy: 0.2100\n",
      "0.29\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.9606 - sparse_categorical_accuracy: 0.2500\n",
      "0.12\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.9803 - sparse_categorical_accuracy: 0.2700\n",
      "0.15\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.9344 - sparse_categorical_accuracy: 0.3000\n",
      "0.18\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.7295 - sparse_categorical_accuracy: 0.3400\n",
      "0.44\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.5314 - sparse_categorical_accuracy: 0.4300\n",
      "0.49\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.3031 - sparse_categorical_accuracy: 0.5100\n",
      "0.51\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.1248 - sparse_categorical_accuracy: 0.5500\n",
      "0.35\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.2407 - sparse_categorical_accuracy: 0.5300\n",
      "0.68\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.2119 - sparse_categorical_accuracy: 0.6800\n",
      "0.55\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.9545 - sparse_categorical_accuracy: 0.7200\n",
      "0.68\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.9453 - sparse_categorical_accuracy: 0.6700\n",
      "0.67\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7182 - sparse_categorical_accuracy: 0.8100\n",
      "0.67\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6981 - sparse_categorical_accuracy: 0.7500\n",
      "0.77\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6290 - sparse_categorical_accuracy: 0.8200\n",
      "0.77\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6306 - sparse_categorical_accuracy: 0.7800\n",
      "0.81\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5651 - sparse_categorical_accuracy: 0.8100\n",
      "0.75\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5000 - sparse_categorical_accuracy: 0.9000\n",
      "0.76\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5105 - sparse_categorical_accuracy: 0.8200\n",
      "0.76\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5156 - sparse_categorical_accuracy: 0.8900\n",
      "0.71\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.8400\n",
      "0.86\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3959 - sparse_categorical_accuracy: 0.9200\n",
      "0.81\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4056 - sparse_categorical_accuracy: 0.8900\n",
      "0.82\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4948 - sparse_categorical_accuracy: 0.8700\n",
      "0.82\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6480 - sparse_categorical_accuracy: 0.7800\n",
      "0.52\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7736 - sparse_categorical_accuracy: 0.7600\n",
      "0.68\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7758 - sparse_categorical_accuracy: 0.7300\n",
      "0.69\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6788 - sparse_categorical_accuracy: 0.7400\n",
      "0.75\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5496 - sparse_categorical_accuracy: 0.8500\n",
      "0.79\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4384 - sparse_categorical_accuracy: 0.9200\n",
      "0.83\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4678 - sparse_categorical_accuracy: 0.8300\n",
      "0.76\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4403 - sparse_categorical_accuracy: 0.9200\n",
      "0.8\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4619 - sparse_categorical_accuracy: 0.8700\n",
      "0.84\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4738 - sparse_categorical_accuracy: 0.8300\n",
      "0.84\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3487 - sparse_categorical_accuracy: 0.9200\n",
      "0.79\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4567 - sparse_categorical_accuracy: 0.9100\n",
      "0.77\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3754 - sparse_categorical_accuracy: 0.9000\n",
      "0.81\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3355 - sparse_categorical_accuracy: 0.9200\n",
      "0.85\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3073 - sparse_categorical_accuracy: 0.9200\n",
      "0.82\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3059 - sparse_categorical_accuracy: 0.8900\n",
      "0.8\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3066 - sparse_categorical_accuracy: 0.9500\n",
      "0.86\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2960 - sparse_categorical_accuracy: 0.9600\n",
      "0.85\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2520 - sparse_categorical_accuracy: 0.9500\n",
      "0.78\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2795 - sparse_categorical_accuracy: 0.9400\n",
      "0.85\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1934 - sparse_categorical_accuracy: 0.9700\n",
      "0.89\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1620 - sparse_categorical_accuracy: 0.9900\n",
      "0.87\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1535 - sparse_categorical_accuracy: 0.9900\n",
      "0.88\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1647 - sparse_categorical_accuracy: 0.9800\n",
      "0.89\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1756 - sparse_categorical_accuracy: 0.9600\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1303 - sparse_categorical_accuracy: 0.9700\n",
      "0.86\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1658 - sparse_categorical_accuracy: 0.9900\n",
      "0.86\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1980 - sparse_categorical_accuracy: 0.9700\n",
      "0.87\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9700\n",
      "0.84\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1114 - sparse_categorical_accuracy: 0.9500\n",
      "0.87\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0706 - sparse_categorical_accuracy: 0.9700\n",
      "0.95\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0630 - sparse_categorical_accuracy: 0.9800\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0534 - sparse_categorical_accuracy: 0.9900\n",
      "0.88\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0507 - sparse_categorical_accuracy: 0.9900\n",
      "0.82\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9900\n",
      "0.92\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0304 - sparse_categorical_accuracy: 1.0000\n",
      "0.89\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0226 - sparse_categorical_accuracy: 1.0000\n",
      "0.89\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0155 - sparse_categorical_accuracy: 1.0000\n",
      "0.89\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0108 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0085 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0074 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0056 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0053 - sparse_categorical_accuracy: 1.0000\n",
      "0.92\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0058 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0046 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0032 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0022 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0016 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0016 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000\n",
      "0.9\n",
      "Accuracy sobre el test set 0.95\n"
     ]
    }
   ],
   "source": [
    "result = 0  #Best score in the training state\n",
    "for i in range(100): #Realizamos tantos ciclos de entrenamiento como se indique en el rango\n",
    "    right=0 #Auxiliar to calculate the accuracy\n",
    "    model.fit(x, y, epochs=1, batch_size=32) #The training state\n",
    "    prediction = model.predict(x_dev) #We predict on the dev sey and calculate the amount of matches out of it\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    for j in range(len(prediction)): \n",
    "        right += (prediction[j]==y_dev[j])\n",
    "    score = right/len(prediction)\n",
    "    print(score)\n",
    "    if(score > result):\n",
    "        best_model = model #We save the model that got best score on the dev set\n",
    "        result = score\n",
    "print(\"Accuracy sobre el test set {:}\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la predicción de la red sobre el test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 de 84\n",
      "1 de 84\n",
      "2 de 84\n",
      "3 de 84\n",
      "4 de 84\n",
      "5 de 84\n",
      "6 de 84\n",
      "7 de 84\n",
      "8 de 84\n",
      "9 de 84\n",
      "10 de 84\n",
      "11 de 84\n",
      "12 de 84\n",
      "13 de 84\n",
      "14 de 84\n",
      "15 de 84\n",
      "16 de 84\n",
      "17 de 84\n",
      "18 de 84\n",
      "19 de 84\n",
      "20 de 84\n",
      "21 de 84\n",
      "22 de 84\n",
      "23 de 84\n",
      "24 de 84\n",
      "25 de 84\n",
      "26 de 84\n",
      "27 de 84\n",
      "28 de 84\n",
      "29 de 84\n",
      "30 de 84\n",
      "31 de 84\n",
      "32 de 84\n",
      "33 de 84\n",
      "34 de 84\n",
      "35 de 84\n",
      "36 de 84\n",
      "37 de 84\n",
      "38 de 84\n",
      "39 de 84\n",
      "40 de 84\n",
      "41 de 84\n",
      "42 de 84\n",
      "43 de 84\n",
      "44 de 84\n",
      "45 de 84\n",
      "46 de 84\n",
      "47 de 84\n",
      "48 de 84\n",
      "49 de 84\n",
      "50 de 84\n",
      "51 de 84\n",
      "52 de 84\n",
      "53 de 84\n",
      "54 de 84\n",
      "55 de 84\n",
      "56 de 84\n",
      "57 de 84\n",
      "58 de 84\n",
      "59 de 84\n",
      "60 de 84\n",
      "61 de 84\n",
      "62 de 84\n",
      "63 de 84\n",
      "64 de 84\n",
      "65 de 84\n",
      "66 de 84\n",
      "67 de 84\n",
      "68 de 84\n",
      "69 de 84\n",
      "70 de 84\n",
      "71 de 84\n",
      "72 de 84\n",
      "73 de 84\n",
      "74 de 84\n",
      "75 de 84\n",
      "76 de 84\n",
      "77 de 84\n",
      "78 de 84\n",
      "79 de 84\n",
      "80 de 84\n",
      "81 de 84\n",
      "82 de 84\n",
      "83 de 84\n"
     ]
    }
   ],
   "source": [
    "num = getFrameNumber() #Cargamos el numero de frames del video\n",
    "\n",
    "test = [] \n",
    "n = int(num//50) #Calculamos de 50 en 50 para evitar colapso en la ram o la memoria de video\n",
    "for i in range(n):\n",
    "    print(\"{:} de {:}\".format(i, n))\n",
    "    frames = getFrames(50)\n",
    "    frames = np.asarray(frames)\n",
    "    predictions = best_model.predict(frames) #Testeamos el modelo sobre el mejor resultado\n",
    "    for j in range(len(predictions)):\n",
    "        test.append(np.argmax(predictions[j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la tasa de acierto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8721428571428571\n"
     ]
    }
   ],
   "source": [
    "gt = pickle.load(open(\"clasificacion_frames.sav\", 'rb')) #Cargamos el ground truth \n",
    "\n",
    "right = 0\n",
    "\n",
    "for i in range(len(test)):\n",
    "    right += test[i]==gt[i]\n",
    "    \n",
    "print(right/len(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
